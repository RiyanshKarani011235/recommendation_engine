{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# References\n",
    "\n",
    "https://www.cs.purdue.edu/homes/lsi/sigir04-cf-norm.pdf\n",
    "\n",
    "http://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf\n",
    "\n",
    "https://www.cs.purdue.edu/homes/lsi/sigir04-cf-norm.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Baseline estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Typical Collaborative Filtering (CF) data exhibit large user and item effects; systematic\n",
    "tendencies for some users to give higher ratings than others,\n",
    "and for some items to receive higher ratings than others. It is customary\n",
    "to adjust the data by accounting for these effects, which we encapsulate \n",
    "within the baseline estimates. Denote by $\\mu$ the overall\n",
    "average rating. A baseline estimate for an unknown rating $r_{ui}$ is\n",
    "denoted by $b_{ui}$ and accounts for the user and item effects:\n",
    "\n",
    "\\begin{equation*}\n",
    "b_{u,i} = \\mu + b_u + b_i\n",
    "\\end{equation*}\n",
    "\n",
    "The parameters $b_u$ and $b_i$ indicate the observer deviations of user $u$ and item $i$ respectively, \n",
    "from the average. For example, suppose\n",
    "that we want a baseline estimate for the rating of the movie Titanic\n",
    "by user Joe. Now, say that the average rating over all movies, µ, is\n",
    "3.7 stars. Furthermore, Titanic is better than an average movie, so it\n",
    "tends to be rated 0.5 stars above the average. On the other hand, Joe\n",
    "is a critical user, who tends to rate 0.3 stars lower than the average.\n",
    "Thus, the baseline estimate for Titanic’s rating by Joe would be 3.9\n",
    "stars by calculating 3.7 − 0.3+0.5. In order to estimate bu and bi\n",
    "one can solve the least squares problem:\n",
    "\n",
    "\\begin{equation*}\n",
    "min_{b_{*}} \\sum_{(u,i)\\subset K} (r_{u,i} - \\mu - b_u - b_i)^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, the first term $\\sum_{(u,i)\\subset K} (r_{u,i} - \\mu - b_u - b_i)^{2}$ strives to find $b_u$'s and $b_i$'s that fit the given ratings. \n",
    "\n",
    "The problem can be formalized as the following (assuming there are $M$ users and $N$ movies) : \n",
    "\n",
    "\\begin{equation*}\n",
    "r_{0, 0} = \\mu + (b_{u_{0}} \\cdot 1 + b_{u_{1}} \\cdot 0 + b_{u_{2}} \\cdot 0 + ... + b_{u_{M}} \\cdot 0) + (b_{i_{0}} \\cdot 1 + b_{i_{1}} \\cdot 0 + b_{i_{2}} \\cdot 0 + ... + b_{i_{N}} \\cdot 0)\\\\\n",
    "r_{0, 1} = \\mu + (b_{u_{0}} \\cdot 1 + b_{u_{1}} \\cdot 0 + b_{u_{2}} \\cdot 0 + ... + b_{u_{M}} \\cdot 0) + (b_{i_{0}} \\cdot 0 + b_{i_{1}} \\cdot 1 + b_{i_{2}} \\cdot 0 + ... + b_{i_{N}} \\cdot 0)\\\\\n",
    "...\\\\\n",
    "r_{1, 0} = \\mu + (b_{u_{0}} \\cdot 0 + b_{u_{1}} \\cdot 1 + b_{u_{2}} \\cdot 0 + ... + b_{u_{M}} \\cdot 0) + (b_{i_{0}} \\cdot 1 + b_{i_{1}} \\cdot 0 + b_{i_{2}} \\cdot 0 + ... + b_{i_{N}} \\cdot 0)\\\\\n",
    "...\\\\\n",
    "r_{M, N} = \\mu + (b_{u_{0}} \\cdot 0 + b_{u_{1}} \\cdot 0 + b_{u_{2}} \\cdot 0 + ... + b_{u_{M}} \\cdot 1) + (b_{i_{0}} \\cdot 0 + b_{i_{1}} \\cdot 0 + b_{i_{2}} \\cdot 0 + ... + b_{i_{N}} \\cdot 1)\n",
    "\\end{equation*}\n",
    "\n",
    "The above set of equations can be more compactly (and more standardly) written as \n",
    "\n",
    "\\begin{equation*}\n",
    "    Y =\n",
    "    \\begin{pmatrix}\n",
    "    r_{0,0} - \\mu\\\\\n",
    "    r_{0,1} - \\mu\\\\\n",
    "    ...\\\\\n",
    "    r_{0,M} - \\mu\\\\\n",
    "    r_{1,0} - \\mu\\\\\n",
    "    r_{1,1} - \\mu\\\\\n",
    "    ...\\\\\n",
    "    r_{N,M} - \\mu\n",
    "    \\end{pmatrix}\n",
    "    X = \n",
    "    \\begin{pmatrix}\n",
    "    1 & 0 & 0 & ... & 0 & 1 & 0 & ... & 0\\\\\n",
    "    1 & 0 & 0 & ... & 0 & 0 & 1 & ... & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots \\\\\n",
    "    1 & 0 & 0 & ... & 0 & 0 & 0 & ... & 1\\\\\n",
    "    0 & 1 & 0 & ... & 0 & 1 & 0 & ... & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots \\\\\n",
    "    0 & 0 & 0 & ... & 1 & 0 & 0 & ... & 1\\\\\n",
    "    \\end{pmatrix}\n",
    "    b = \n",
    "    \\begin{pmatrix}\n",
    "    b_{u_{0}}\\\\\n",
    "    b_{u_{1}}\\\\\n",
    "    ...\\\\\n",
    "    b_{u_{M}}\\\\\n",
    "    b_{i_{0}}\\\\\n",
    "    b_{i_{1}}\\\\\n",
    "    ...\\\\\n",
    "    b_{i_{N}}\n",
    "    \\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "The relationship between these three matrices give us the formalized representation of the problem\n",
    "\n",
    "\\begin{equation*}\n",
    "Y = X * b\\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "So, our problem can now be restated to finding a configuration of the parameter matrix $b$ that minimizes the squared error in equation 1. The solution to this problem is straightforward when using Linear Regression, by which the parameter matrix that satisfies the conditions in equation 1 can be given as :\n",
    "\n",
    "\\begin{equation*}\n",
    "b = (X^{T}X)^{-1}X^{T}Y\\tag{3}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Baseline estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as sm\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "import os\n",
    "import random\n",
    "\n",
    "# dataset root\n",
    "root_dataset_folder = os.path.join('..', 'data', 'ml-100k')\n",
    "\n",
    "# u.user (Users)\n",
    "user_columns = [\n",
    "    'user_id', \n",
    "    'age', \n",
    "    'sex', \n",
    "    'occupation', \n",
    "    'zip_code'\n",
    "]\n",
    "users = pd.read_csv(os.path.join(root_dataset_folder, 'u.user'), sep='|', names=user_columns, encoding='latin-1')\n",
    "\n",
    "# u.items (Movies)\n",
    "movie_columns = [\n",
    "    'movie_id', \n",
    "    'movie_title' ,\n",
    "    'release_date',\n",
    "    'video_release_date', \n",
    "    'IMDb_URL', \n",
    "    'unknown', \n",
    "    'Action', \n",
    "    'Adventure', \n",
    "    'Animation', \n",
    "    'Children\\'s', \n",
    "    'Comedy', \n",
    "    'Crime', \n",
    "    'Documentary', \n",
    "    'Drama', \n",
    "    'Fantasy', \n",
    "    'Film-Noir', \n",
    "    'Horror', \n",
    "    'Musical', \n",
    "    'Mystery', \n",
    "    'Romance', \n",
    "    'Sci-Fi', \n",
    "    'Thriller', \n",
    "    'War',\n",
    "    'Western'\n",
    "]\n",
    "movies = pd.read_csv(os.path.join(root_dataset_folder, 'u.item'), sep='|', names=movie_columns, encoding='latin-1')\n",
    "\n",
    "#Creating sparse_rating_matrix:\n",
    "M = users.user_id.unique().shape[0]\n",
    "N = movies.movie_id.unique().shape[0]\n",
    "\n",
    "ratings = np.zeros((M, N))\n",
    "# reading ratings from \n",
    "# (Ratings)\n",
    "rating_columns = [\n",
    "    'user_id', \n",
    "    'movie_id', \n",
    "    'rating', \n",
    "    'unix_timestamp'\n",
    "]\n",
    "# ua.base\n",
    "data_base = pd.read_csv(os.path.join(root_dataset_folder, 'ua.base'), sep='\\t', names=rating_columns, encoding='latin-1')\n",
    "\n",
    "for row in data_base.itertuples():\n",
    "    ratings[row[1]-1 , row[2] -1] = row[3]\n",
    "\n",
    "sparse_rating_matrix = pd.DataFrame(data=ratings,index=users['user_id'],columns=movies['movie_id'])\n",
    "sparse_rating_matrix = sparse_rating_matrix.replace('0',np.nan)\n",
    "sparse_rating_matrix = sparse_rating_matrix.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In our dataset, the Sparse Rating matrix being very sparse (hence the name), a lot of the entries in the matrix $Y$ will be NaN. This will lead to most of the entries in the parameter matrix $b$ to also result in NaN. To overcome this problem, we remove the rows in the matrix $Y$ with values NaN, and the corresponding rows in matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# generating the X and Y matrices, for linear regression\n",
    "Y = []\n",
    "X = []\n",
    "mean = np.nanmean(sparse_rating_matrix)\n",
    "for m in range(M) : \n",
    "    for n in range(N) : \n",
    "        if not np.isnan(sparse_rating_matrix[m][n]) : \n",
    "            Y.append(sparse_rating_matrix[m][n] - mean)\n",
    "            x = np.zeros(M+N)\n",
    "            x[m] = 1\n",
    "            x[M + n] = 1\n",
    "            X.append(x)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# then \"lstsq\" function performs the least squares linear regression\n",
    "b = np.linalg.lstsq(X, Y)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Regularization in linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "These parameters represent the optimum solution to equation 2, but the values here are very large. In linear regression, large values represent overfitting and we certainly don't want our model to overfit. For this reason, we introduce a regularization term in equation 1\n",
    "\n",
    "\\begin{equation*}\n",
    "b_{u,i} = \\mu + b_u + b_i + \\lambda_{1} (\\sum_{u} b_u^2 + \\sum_{i} b_i^2)\\tag{4}. \n",
    "\\end{equation*}\n",
    "\n",
    "Ther regularizing term - $\\lambda_{1} (\\sum_{u} b_u^2 + \\sum_{i} b_i^2)$ - avoids overfitting by penalizing the magnitudes of the parameters.\n",
    "\n",
    "In terms of linear regression, this regularization terms can be incorporated by modifying equation 3 by writing so ([link](http://eniac.cs.qc.cuny.edu/andrew/gcml/lecture5.pdf))\n",
    "\n",
    "\\begin{equation*}\n",
    "b = (X^{T}X - \\lambda I)^{-1}X^{T}Y\\tag{3}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "let\\ t_1 = (X^{T}X - \\lambda I)^{-1}, t_2 = X^{T}Y\\\\\n",
    "then,\\ b = t_1\\cdot t_2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambda_ = 0.1\n",
    "X_transpose = np.transpose(X)\n",
    "t1 = np.linalg.inv(np.dot(X_transpose, X) - (lambda_*np.identity(X.shape[1])))\n",
    "t2 = np.dot(X_transpose, Y)\n",
    "b = np.dot(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "b_u = b[:M]\n",
    "b_i = b[M:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910223621593\n"
     ]
    }
   ],
   "source": [
    "def rmse() : \n",
    "    error = 0\n",
    "    count = 0\n",
    "    for m in range(M) : \n",
    "        for n in range(N) : \n",
    "            if not np.isnan(sparse_rating_matrix[m][n]) : \n",
    "                err = sparse_rating_matrix[m][n] - mean - b_u[m] - b_i[n]\n",
    "                error += err**2\n",
    "                count += 1\n",
    "    \n",
    "    error = (error/count)**0.5\n",
    "    return error\n",
    "\n",
    "print(rmse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we can build a matrix $B$ called the baseline matrix, which is basically the matrix of the baseline predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# generating baseline estimate matrix\n",
    "baseline_estimate_matrix = np.zeros((M, N))\n",
    "for m in range(M) : \n",
    "        for n in range(N) : \n",
    "            if not np.isnan(sparse_rating_matrix[m][n]) : \n",
    "                estimate = mean + b_u[m] + b_i[n]\n",
    "                baseline_estimate_matrix[m][n] = estimate \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Baseline estimates error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# error matrix will be the element wise difference of the \n",
    "# baseline estimate matrix and the sparse rating matrix\n",
    "error_vector = []\n",
    "for m in range(M) : \n",
    "    for n in range(N) : \n",
    "        if not np.isnan(sparse_rating_matrix[m][n]) : \n",
    "            error_vector.append(abs(sparse_rating_matrix[m][n] - mean - b_u[m] - b_i[n]))\n",
    "error_vector = np.array(error_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118da2fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "error_vectors_list = [[element for element in error_vector[error_vector <= 0.5]]]\n",
    "error_vectors_list.append([element for element in error_vector[(error_vector <= 1) & (error_vector > 0.5)]])\n",
    "error_vectors_list.append([element for element in error_vector[(error_vector <= 1.5) & (error_vector > 1)]])\n",
    "error_vectors_list.append([element for element in error_vector[(error_vector <= 2) & (error_vector > 1.5)]])\n",
    "error_vectors_list.append([element for element in error_vector[(error_vector <= 2.5) & (error_vector > 2)]])\n",
    "error_vectors_list.append([element for element in error_vector[(error_vector <= 3) & (error_vector > 2.5)]])\n",
    "error_vectors_list.append([element for element in error_vector[(error_vector <= 3.5) & (error_vector > 3)]])\n",
    "\n",
    "plt.plot([0.5, 1, 1.5, 2, 2.5, 3, 3.5], [len(element) for element in error_vectors_list])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Item based Nearest Neighbourhood model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Central to most item-oriented approaches is a similarity measure\n",
    "between items. Frequently, it is based on the Pearson correlation\n",
    "coefficient, $\\rho_{ij}$ , which measures the tendency of users to rate items\n",
    "$i$ and $j$ similarly. Since many ratings are unknown, it is expected\n",
    "that some items share only a handful of common raters. Computation\n",
    "of the correlation coefficient is based only on the common user\n",
    "support. Accordingly, similarities based on a greater user support\n",
    "are more reliable. An appropriate similarity measure, denoted by\n",
    "$s_{ij}$, would be a shrunk correlation coefficient\n",
    "\n",
    "\\begin{equation*}\n",
    "s_{ij} = \\frac{n_{ij}}{n_{ij} + \\lambda_{2}} \\cdot \\rho_{ij}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $\\rho_{ij}$ is the <b>Pearson Corellation Coefficient</b> given by [(link)](http://www.cs.carleton.edu/cs_comps/0607/recommend/recommender/itembased.html)\n",
    "\\begin{equation*}\n",
    "\\rho_{ij} = \\frac{\\sum_{u \\in U} (R_{u,i} - \\overline R_i)(R_{u,j} - \\overline R_j)}{\\sqrt{\\sum_{u \\in U} (R_{u,i} - \\overline R_i)^2} \\sqrt{\\sum_{u \\in U} (R_{u,j} - \\overline R_j)^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "$R_{u,i}$ is the rating that user $u$ has given to item $i$, and $\\overline R_i$ is the average rating for item $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Involving features for better similarity estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Though the Pearson Corellation coefficient is a very standard similarity measure, it does not include any information from the features already available. We have, in our dataset, information about the genre of each movies, and this should (intutively) be a very good addition to the similarity of two movies. To include this information in our model, we modify the Pearson Corellation Coefficient like follows\n",
    "\n",
    "\\begin{equation*}\n",
    "\\overline \\rho_{ij} = w_1 \\cdot \\frac{\\sum_{u \\in U} (R_{u,i} - \\overline R_i)(R_{u,j} - \\overline R_j)}{\\sqrt{\\sum_{u \\in U} (R_{u,i} - \\overline R_i)^2} \\sqrt{\\sum_{u \\in U} (R_{u,j} - \\overline R_j)^2}} + w_2 \\cdot G_{i,j}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, the term $G_{i,j}$ is the similarity of movies based only on the genres that the movies belong to. For example, the genres <b>Animation</b> and <b>Children's</b> are similar, but the genres <b>Action</b> and <b>Documentary</b> are quite different. This can also be seen from the movie genre data that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv(os.path.join(root_dataset_folder, 'u.item'), sep='|', names=movie_columns, encoding='latin-1')\n",
    "genre_matrix = movies.drop('movie_id', 1).drop('movie_title', 1).drop('release_date', 1).drop('video_release_date', 1).drop('IMDb_URL', 1).drop('unknown', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to accomodate such observations into our estimate of $G_{ij}$, we need to build a model that encompasses such relations between genres. A model such as below should do well\n",
    "\n",
    "\\begin{equation*}\n",
    "g_{ij} = \\frac{w_1 \\cdot (\\# \\; times \\; g_i \\; and \\; g_j \\; occured \\; in \\; the \\; same \\; movie) - w_2 \\cdot (\\# \\; times \\; only\\; one \\; of \\; g_i \\; and \\; g_j \\; occured \\; in \\; a \\; movie)}\n",
    "{max \\; (g_{ij})}\n",
    "\\end{equation*}\n",
    "\n",
    "This should give us a genre similarity rating between -1 and 1, <b>-1</b> being very different and <b>1</b> being very similar. We could then build a matrix that would give us the similarity between two genres. An upper triangular matrix of the form as show below should suffice\n",
    "\n",
    "\\begin{equation*}\n",
    "    g = \n",
    "    \\begin{pmatrix}\n",
    "    0 & g_{01} & g_{02} & g_{03} & ... & g_{0,n-1} & g_{0n}\\\\\n",
    "    0 & 0 & g_{12} & g_{13} &  ... & g_{1,n-1} & g_{1n}\\\\\n",
    "    0 & 0 & 0 & g{23} & ... & g_{2,n-1} & g_{2n}\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "    0 & 0 & 0 & 0 & ... & g_{n-2,n-1} & g_{n-2,n}\\\\\n",
    "    0 & 0 & 0 & 0 & ... & 0 & g_{n-1, n}\n",
    "    \\end{pmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "w1 = 1.0                    # amount of reward added when both ratings are 1\n",
    "w2 = 0.675                  # amount of reward subtracted when one rating is 1 and another rating is 0\n",
    "w3 = 0.3                    # amount of reward added when both ratings are 0\n",
    "\n",
    "g_matrix = genre_matrix.values\n",
    "g = np.array([np.array([0.0 for i in range(g_matrix.shape[1])]) for j in range(g_matrix.shape[1])])\n",
    "count = 0\n",
    "for movie_genres in g_matrix : \n",
    "    for i in range(len(movie_genres)) :\n",
    "        for j in range(i+1, len(movie_genres)) : \n",
    "            ij = (movie_genres[i], movie_genres[j])\n",
    "            if ij == (0, 1) or ij == (1, 0) :\n",
    "                g[i][j] -= w2 * 1.0\n",
    "            elif ij == (1, 1) : \n",
    "                g[i][j] += w1 * 1.0\n",
    "            elif ij == (0, 0) : \n",
    "                g[i][j] += w3\n",
    "            count += 1.0\n",
    "\n",
    "# normalizing the g matrix with values between -1 and 1\n",
    "max_val = max(g.min(), g.max(), key=abs)\n",
    "for i in range(len(g)) : \n",
    "    for j in range(len(g)) : \n",
    "        g[i][j] /= max_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After this, finding the value of $g_{ij}$ for a pair of movies $(i,j)$ is simply a matter of adding the $g_{mn}$ values of all the pair of genres $(m,n) \\in (m \\neq 0 \\in genre-matrix[i], \\; n \\neq 0 \\in genre-matrix[j])$ \n",
    "\n",
    "\\begin{equation*}\n",
    "G_{ij} = \\frac{1}{N} \\sum g_{mn} \\; \\forall \\; \\{(m,n) \\; | \\; m \\neq 0 \\in genre \\; matrix[i], \\; n \\neq 0 \\in genre \\; matrix[j]\\}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = np.array([np.array([0.0 for i in range(g_matrix.shape[0])]) for j in range(g_matrix.shape[0])])\n",
    "\n",
    "def G_ij(i, j) : \n",
    "    sum_ = 0\n",
    "    genre_matrix_i = g_matrix[i]\n",
    "    genre_matrix_j = g_matrix[j]\n",
    "    count = 0\n",
    "    for m in range(len(genre_matrix_i)) :\n",
    "        ei = genre_matrix_i[m]\n",
    "        if ei : \n",
    "            for n in range(len(genre_matrix_j)) :\n",
    "                ej = genre_matrix_j[n]\n",
    "                if ej : \n",
    "                    sum_ += g[m][n]\n",
    "                    count += 1\n",
    "    if count is not 0 : \n",
    "        sum_ /= count\n",
    "    return sum_\n",
    "\n",
    "for i in range(len(g_matrix)) : \n",
    "    for j in range(i+1, len(g_matrix)) : \n",
    "        G[i][j] = G_ij(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for any movie-movie pair (i,j), n[i][j] will indicate the\n",
    "# number of users that have rated both the movies\n",
    "n = np.zeros((sparse_rating_matrix.shape[1], sparse_rating_matrix.shape[1]))\n",
    "for i in range(sparse_rating_matrix.shape[1]) : \n",
    "    for j in range(i+1, sparse_rating_matrix.shape[1]) : \n",
    "        R_i = sparse_rating_matrix[:, i]\n",
    "        R_j = sparse_rating_matrix[:, j]\n",
    "        R_i = np.nan_to_num(R_i)\n",
    "        R_j = np.nan_to_num(R_j)\n",
    "        R_i[R_i != 0] = 1\n",
    "        R_j[R_j != 0] = 1\n",
    "        n[i][j] = np.dot(R_i, R_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def r(u, i) : \n",
    "    return sparse_rating_matrix[u][i]\n",
    "\n",
    "# r_i\n",
    "r_i_value = sparse_rating_matrix.mean(axis=0).tolist()\n",
    "def r_i(u) : \n",
    "    return r_i_value[u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ironstein/anaconda/envs/recommendation_engine/lib/python2.7/site-packages/ipykernel/__main__.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/ironstein/anaconda/envs/recommendation_engine/lib/python2.7/site-packages/ipykernel/__main__.py:8: RuntimeWarning: Mean of empty slice\n",
      "/Users/ironstein/anaconda/envs/recommendation_engine/lib/python2.7/site-packages/ipykernel/__main__.py:6: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "rho = np.zeros((sparse_rating_matrix.shape[1], sparse_rating_matrix.shape[1]))\n",
    "for i in range(sparse_rating_matrix.shape[1]) : \n",
    "    for j in range(i+1, sparse_rating_matrix.shape[1]) : \n",
    "        num = 0\n",
    "        R_i = sparse_rating_matrix[:, i]\n",
    "        R_i = R_i - np.nanmean(R_i, axis=0)\n",
    "        R_j = sparse_rating_matrix[:, j]\n",
    "        R_j = R_j - np.nanmean(R_j, axis=0)\n",
    "        R_i = np.nan_to_num(R_i)\n",
    "        R_j = np.nan_to_num(R_j)\n",
    "        numerator = np.dot(R_i, R_j)\n",
    "        denominator = np.dot(R_i, R_i) * np.dot(R_j, R_j)\n",
    "        val = numerator/denominator\n",
    "        if np.isnan(val) : \n",
    "            rho[i][j] = 0\n",
    "        else :     \n",
    "            rho[i][j] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w1 = 1\n",
    "w2 = 1\n",
    "rho_ = (w1*rho) + (w2*G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambda_2 = 0\n",
    "s = (n/(n + 100)) * rho_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Recommendation for a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our goal is to predict rui – the unobserved rating by user u for\n",
    "item i. Using the similarity measure, we identify the k items rated\n",
    "by u, which are most similar to i. This set of k neighbors is denoted\n",
    "by $S^k_{(i; u)}$. The predicted value of rui is taken as a weighted average\n",
    "of the ratings of neighboring items, while adjusting for user\n",
    "and item effects through the baseline estimates:\n",
    "$rˆ_{ui} = b_{ui} + \\frac{\\sum{j∈S_k(i;u)} s_{ij} (r_{uj} − b_{uj} )}{j \\in S^k_{(i;u)} s_{ij}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sparse_rating_matrix = pd.DataFrame(data=ratings,index=users['user_id'],columns=movies['movie_id'])\n",
    "sparse_rating_matrix = sparse_rating_matrix.replace('0',np.nan)\n",
    "sparse_rating_matrix = sparse_rating_matrix.values\n",
    "k = 15 # number of nearest neighbours to consider for each movie\n",
    "\n",
    "# returns the indices of k max elements in the array \n",
    "# @param {k} : number of indices to return\n",
    "# @param {array} : the array in which to look for the k max elements\n",
    "def k_max_indices(array, k) : \n",
    "    return_array = []\n",
    "    for i in range(k) : \n",
    "        max_ = -1000\n",
    "        max_index = -1\n",
    "        for j in range(len(array)) : \n",
    "            if array[j] > max_ and j not in return_array : \n",
    "                max_ = array[j]\n",
    "                max_index = j\n",
    "        return_array.append(max_index)\n",
    "    return return_array\n",
    "\n",
    "# returns the k most similar movies to the movie \"i\"\n",
    "# @param {k} : the number of nearest neighbours to return\n",
    "# @param {i} : the movie for which to look for nearest neighbours\n",
    "def knn(k, i) :\n",
    "    row = s[i][i+1:].tolist()\n",
    "    array = s[:,i][:i].tolist()\n",
    "    array.append(0)\n",
    "    array.extend(row)\n",
    "    return k_max_indices(array, k)\n",
    "\n",
    "# takes in a user id, and looks at the 5 highest rated movies by that user\n",
    "# then, finds k nearest neighbours for all the 5 movies and returns the\n",
    "# 5 highest rated movies from that list\n",
    "# @param {user_id} : the user for whom to recommend movies\n",
    "def recommend(user_id) : \n",
    "    user_rating_matrix = sparse_rating_matrix[user_id]\n",
    "    user_rating_matrix[user_rating_matrix == np.nan] = 0\n",
    "    max_user_rating_matrix = k_max_indices(user_rating_matrix, 5)\n",
    "    m = []\n",
    "    for element in max_user_rating_matrix : \n",
    "        m.extend([[element, e] for e in knn(5, element)])\n",
    "    m_ = [s[u,v] for u,v in m]\n",
    "    m_ = k_max_indices(m_, k)[7:12]\n",
    "    m = [m[element] for element in m_]\n",
    "    recommended_movies_array = []\n",
    "    with open(os.path.join(root_dataset_folder, 'u.item'), 'r') as f : \n",
    "        data = f.read().split('\\n')\n",
    "        for movie in m : \n",
    "            recommended_movies_array.append(data[movie[1]].split('|')[1])\n",
    "    return recommended_movies_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Game, The (1997)',\n",
       " 'Tombstone (1993)',\n",
       " \"Singin' in the Rain (1952)\",\n",
       " 'My Fair Lady (1964)',\n",
       " 'Annie Hall (1977)']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend(104)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
